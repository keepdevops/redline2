"""
Analysis routes for REDLINE Web GUI
Route handlers for data analysis and statistical operations

This module now serves as a central registry for all analysis blueprints.
Individual route modules are organized by functionality.
"""

from flask import Blueprint

# Import all analysis blueprints
from .analysis_tab import analysis_tab_bp
from .analysis_export import analysis_export_bp

# Main analysis blueprint - registers all sub-blueprints
analysis_bp = Blueprint('analysis', __name__)

# Register all sub-blueprints with the main analysis blueprint
# Use url_prefix to maintain route structure
analysis_bp.register_blueprint(analysis_tab_bp, url_prefix='')
analysis_bp.register_blueprint(analysis_export_bp, url_prefix='')

def clean_dataframe_columns(df):
    """Clean up malformed CSV headers - remove unnamed/empty columns."""
    columns_to_drop = []
    cleaned_columns = []
    
    for i, col in enumerate(df.columns):
        # Drop columns with empty names or typical pandas unnamed column patterns
        if (col == '' or 
            str(col).strip() == '' or 
            str(col).startswith('Unnamed:') or
            (str(col) == '0' and i == 0 and len(df.columns) > 1)):  # First column named '0' usually indicates index issue
            columns_to_drop.append(col)
        else:
            # Clean column name
            clean_col = str(col).strip()
            # If still empty after cleaning, give it a meaningful name
            if clean_col == '':
                clean_col = f'Column_{i}'
            cleaned_columns.append(clean_col)
    
    # Drop the problematic columns
    if columns_to_drop:
        df = df.drop(columns=columns_to_drop)
        
    # Rename columns to cleaned versions
    if len(cleaned_columns) == len(df.columns):
        df.columns = cleaned_columns
        
    return df

def convert_numpy_types(obj):
    """Convert numpy types to native Python types for JSON serialization."""
    import math
    
    # Handle None
    if obj is None:
        return None
    
    # Handle dicts
    if isinstance(obj, dict):
        return {str(k): convert_numpy_types(v) for k, v in obj.items()}
    
    # Handle lists
    if isinstance(obj, list):
        return [convert_numpy_types(item) for item in obj]
    
    # Handle numpy integers
    if isinstance(obj, (np.integer, np.int64, np.int32, np.int16, np.int8)):
        return int(obj)
    
    # Handle numpy floats
    if isinstance(obj, (np.floating, np.float64, np.float32)):
        val = float(obj)
        # Replace NaN and infinity with None or 0
        if math.isnan(val) or math.isinf(val):
            return None
        return val
    
    # Handle regular floats with NaN/inf
    if isinstance(obj, float):
        if math.isnan(obj) or math.isinf(obj):
            return None
        return obj
    
    # Handle numpy arrays
    if isinstance(obj, np.ndarray):
        return [convert_numpy_types(item) for item in obj.tolist()]
    
    # Handle pandas objects
    if isinstance(obj, pd.Series):
        return [convert_numpy_types(v) for v in obj.tolist()]
    
    if isinstance(obj, pd.DataFrame):
        return obj.to_dict(orient='records')
    
    # Handle basic types that are JSON serializable
    if isinstance(obj, (str, int, bool)):
        return obj
    
    # Fallback: convert to string for anything else
    try:
        return str(obj)
    except:
        return None

@analysis_bp.route('/')
def analysis_tab():
    """Analysis tab main page."""
    return render_template('analysis_tab.html')

@analysis_bp.route('/analyze', methods=['POST'])
def analyze_data():
    """Perform analysis on loaded data."""
    try:
        data = request.get_json()
        filename = data.get('filename')
        analysis_type = data.get('analysis_type', 'basic')
        
        if not filename:
            return jsonify({'error': 'No filename provided'}), 400
        
        from redline.core.format_converter import FormatConverter
        from redline.core.schema import EXT_TO_FORMAT
        
        converter = FormatConverter()
        
        # Determine file path - check multiple locations including converted files
        data_dir = os.path.join(os.getcwd(), 'data')
        data_path = None
        
        # If file_path is provided (from converted files), use it directly
        file_path_hint = data.get('file_path')
        if file_path_hint and os.path.exists(file_path_hint):
            data_path = file_path_hint
        else:
            # Check locations in order of priority (same as /data/load endpoint):
            # 1. Root data directory
            # 2. data/stooq directory (for Stooq downloads)
            # 3. data/downloaded directory (for other downloads)
            # 4. data/uploads directory (for uploaded files)
            # 5. data/converted directory (recursively, for converted files)
            search_paths = [
                os.path.join(data_dir, filename),
                os.path.join(data_dir, 'stooq', filename),
                os.path.join(data_dir, 'downloaded', filename),
                os.path.join(data_dir, 'uploads', filename)
            ]
            
            # Check converted directory recursively
            converted_dir = os.path.join(data_dir, 'converted')
            if os.path.exists(converted_dir):
                for root, dirs, files in os.walk(converted_dir):
                    if filename in files:
                        search_paths.append(os.path.join(root, filename))
            
            # Try each path
            for path in search_paths:
                if os.path.exists(path):
                    data_path = path
                    break
        
        if not data_path or not os.path.exists(data_path):
            return jsonify({'error': f'File not found: {filename}'}), 404
        
        # Detect format from file extension (same as Tkinter)
        ext = os.path.splitext(data_path)[1].lower()
        format_type = EXT_TO_FORMAT.get(ext, 'csv')
        
        # Use direct pandas loading for speed (same as data routes)
        if format_type == 'csv':
            df = pd.read_csv(data_path)
        elif format_type == 'txt':
            # Try different separators for TXT files
            df = None
            for sep in [',', '\t', ';', ' ', '|']:
                try:
                    test_df = pd.read_csv(data_path, sep=sep)
                    # Check if we got multiple columns (good parsing)
                    if len(test_df.columns) > 1:
                        df = test_df
                        break
                except:
                    continue
            
            if df is None:
                # If all separators fail, try reading as fixed-width
                df = pd.read_fwf(data_path)
        elif format_type == 'parquet':
            df = pd.read_parquet(data_path)
        elif format_type == 'feather':
            df = pd.read_feather(data_path)
        elif format_type == 'json':
            df = pd.read_json(data_path)
        elif format_type == 'duckdb':
            import duckdb
            conn = duckdb.connect(data_path)
            df = conn.execute("SELECT * FROM tickers_data").fetchdf()
            conn.close()
        else:
            # Fallback to converter for unsupported formats
            df = converter.load_file_by_type(data_path, format_type)
        
        if not isinstance(df, pd.DataFrame):
            return jsonify({'error': 'Invalid data format'}), 400
        
        # Clean up malformed CSV headers - remove unnamed/empty columns
        df = clean_dataframe_columns(df)
        
        analysis_result = {}
        
        if analysis_type == 'basic':
            analysis_result = perform_basic_analysis(df)
        elif analysis_type == 'financial':
            analysis_result = perform_financial_analysis(df)
        elif analysis_type == 'statistical':
            analysis_result = perform_statistical_analysis(df)
        elif analysis_type == 'correlation':
            analysis_result = perform_correlation_analysis(df)
        else:
            return jsonify({'error': f'Unknown analysis type: {analysis_type}'}), 400
        
        # Convert all numpy types in the result
        cleaned_result = convert_numpy_types(analysis_result)
        
        return jsonify({
            'filename': filename,
            'analysis_type': analysis_type,
            'result': cleaned_result,
            'data_shape': df.shape,
            'columns': list(df.columns)
        })
        
    except ValueError as ve:
        logger.error(f"Value error in analysis: {str(ve)}")
        return jsonify({'error': f'Value error: {str(ve)}'}), 500
    except Exception as e:
        logger.error(f"Error performing analysis: {str(e)}")
        return jsonify({'error': str(e)}), 500

def perform_basic_analysis(df):
    """Perform basic data analysis."""
    try:
        analysis = {
            'shape': {
                'rows': int(len(df)),
                'columns': int(len(df.columns))
            },
            'data_types': df.dtypes.astype(str).to_dict(),
            'null_counts': convert_numpy_types(df.isnull().sum().to_dict()),
            'memory_usage': int(df.memory_usage(deep=True).sum()),
            'numeric_summary': {},
            'categorical_summary': {}
        }
        
        # Exclude date/datetime columns from numeric analysis
        date_keywords = ['date', 'time', 'timestamp', 'year', 'month', 'day']
        date_cols = [col for col in df.columns 
                    if any(keyword in str(col).lower() for keyword in date_keywords) 
                    or pd.api.types.is_datetime64_any_dtype(df[col])]
        
        # Also exclude columns that are likely date components (year, month, day as integers)
        # Check if column values look like dates (years 1900-2100, months 1-12, days 1-31)
        for col in df.columns:
            if col not in date_cols and pd.api.types.is_integer_dtype(df[col]):
                unique_vals = df[col].dropna().unique()
                if len(unique_vals) > 0:
                    min_val, max_val = unique_vals.min(), unique_vals.max()
                    # Check if values look like years, months, or days
                    if (col.lower() == 'year' or (min_val >= 1900 and max_val <= 2100 and len(unique_vals) <= 100)):
                        date_cols.append(col)
                    elif (col.lower() == 'month' or (min_val >= 1 and max_val <= 12 and len(unique_vals) <= 12)):
                        date_cols.append(col)
                    elif (col.lower() == 'day' or (min_val >= 1 and max_val <= 31 and len(unique_vals) <= 31)):
                        date_cols.append(col)
        
        # Also exclude columns with very large numbers that could be timestamps
        # Unix timestamps (seconds): 1000000000-2000000000 (2001-2033)
        # Unix timestamps (milliseconds): 1000000000000-2000000000000
        # Excel serial dates: 40000-50000 (around 2009-2037)
        for col in df.columns:
            if col not in date_cols and pd.api.types.is_numeric_dtype(df[col]):
                numeric_vals = df[col].dropna()
                if len(numeric_vals) > 0:
                    min_val, max_val = numeric_vals.min(), numeric_vals.max()
                    # Check for Unix timestamps (seconds) - 1000000000 to 2000000000
                    if min_val >= 1000000000 and max_val <= 2000000000:
                        date_cols.append(col)
                        logger.debug(f"Excluding column '{col}' as Unix timestamp (seconds): range {min_val}-{max_val}")
                    # Check for Unix timestamps (milliseconds) - 1000000000000 to 2000000000000
                    elif min_val >= 1000000000000 and max_val <= 2000000000000:
                        date_cols.append(col)
                        logger.debug(f"Excluding column '{col}' as Unix timestamp (milliseconds): range {min_val}-{max_val}")
                    # Check for Excel serial dates - 40000 to 50000
                    elif min_val >= 40000 and max_val <= 50000:
                        date_cols.append(col)
                        logger.debug(f"Excluding column '{col}' as Excel serial date: range {min_val}-{max_val}")
                    # Check for very large numbers that are suspiciously date-like
                    # If values are in millions and look like they could be timestamps
                    # Be more aggressive: if all values are in this range and there are many unique values,
                    # it's likely a timestamp column (even without date-like name)
                    elif min_val > 10000000 and max_val < 1000000000:
                        unique_count = len(numeric_vals.unique())
                        total_count = len(numeric_vals)
                        # If most values are unique and in suspicious range, likely timestamps
                        if unique_count > 50 and (unique_count / total_count) > 0.8:
                            date_cols.append(col)
                            logger.debug(f"Excluding column '{col}' as potential timestamp: range {min_val:.2f}-{max_val:.2f}, {unique_count} unique values")
                        # Also exclude if column name suggests it's a date
                        elif any(keyword in str(col).lower() for keyword in ['time', 'date', 'stamp', 'epoch']):
                            date_cols.append(col)
                            logger.debug(f"Excluding column '{col}' as potential timestamp based on name and value range")
        
        # Numeric columns analysis (excluding date columns)
        numeric_cols = [col for col in df.select_dtypes(include=[np.number]).columns 
                       if col not in date_cols]
        
        if len(numeric_cols) > 0:
            numeric_df = df[numeric_cols]
            
            analysis['numeric_summary'] = {
                'count': convert_numpy_types(numeric_df.count().to_dict()),
                'mean': convert_numpy_types(numeric_df.mean().to_dict()),
                'std': convert_numpy_types(numeric_df.std().to_dict()),
                'min': convert_numpy_types(numeric_df.min().to_dict()),
                'max': convert_numpy_types(numeric_df.max().to_dict()),
                'percentiles': {
                    '25%': convert_numpy_types(numeric_df.quantile(0.25).to_dict()),
                    '50%': convert_numpy_types(numeric_df.quantile(0.50).to_dict()),
                    '75%': convert_numpy_types(numeric_df.quantile(0.75).to_dict())
                }
            }
        
        # Categorical columns analysis
        categorical_cols = df.select_dtypes(include=['object', 'category']).columns
        if len(categorical_cols) > 0:
            for col in categorical_cols:
                analysis['categorical_summary'][col] = {
                    'unique_count': int(df[col].nunique()),
                    'most_common': convert_numpy_types(df[col].value_counts().head(5).to_dict()),
                    'null_count': int(df[col].isnull().sum())
                }
        
        return analysis
        
    except Exception as e:
        logger.error(f"Error in basic analysis: {str(e)}")
        return {'error': str(e)}

def perform_financial_analysis(df):
    """Perform financial data analysis."""
    try:
        analysis = {
            'price_analysis': {},
            'volume_analysis': {},
            'returns_analysis': {},
            'volatility_analysis': {}
        }
        
        # Common financial column names - enhanced detection
        price_cols = [col for col in df.columns if any(price in col.lower() for price in ['close', 'price', 'adj close', 'px_last', '<close>', 'c'])]
        volume_cols = [col for col in df.columns if any(vol in col.lower() for vol in ['volume', 'vol', '<vol>', 'px_volume', 'v'])]
        high_cols = [col for col in df.columns if any(high in col.lower() for high in ['high', '<high>', 'px_high', 'h'])]
        low_cols = [col for col in df.columns if any(low in col.lower() for low in ['low', '<low>', 'px_low', 'l'])]
        open_cols = [col for col in df.columns if any(open in col.lower() for open in ['open', '<open>', 'px_open', 'o'])]
        
        if price_cols:
            price_col = price_cols[0]  # Use first price column
            prices = pd.to_numeric(df[price_col], errors='coerce')
            
            analysis['price_analysis'] = {
                    'current_price': convert_numpy_types(prices.iloc[-1]) if not prices.empty else None,
                    'price_range': {
                        'min': convert_numpy_types(prices.min()),
                        'max': convert_numpy_types(prices.max()),
                        'avg': convert_numpy_types(prices.mean())
                    },
                    'price_change': {
                        'absolute': convert_numpy_types(prices.iloc[-1] - prices.iloc[0]) if len(prices) > 1 else 0,
                        'percentage': convert_numpy_types(((prices.iloc[-1] - prices.iloc[0]) / prices.iloc[0]) * 100) if len(prices) > 1 and prices.iloc[0] != 0 else 0
                    }
                }
            
            # Calculate returns
            returns = prices.pct_change().dropna()
            if not returns.empty:
                analysis['returns_analysis'] = {
                    'mean_return': convert_numpy_types(returns.mean()),
                    'std_return': convert_numpy_types(returns.std()),
                    'total_return': convert_numpy_types((prices.iloc[-1] / prices.iloc[0] - 1) * 100) if prices.iloc[0] != 0 else 0,
                    'sharpe_ratio': convert_numpy_types(returns.mean() / returns.std()) if returns.std() != 0 else 0
                }
                
                analysis['volatility_analysis'] = {
                    'daily_volatility': convert_numpy_types(returns.std()),
                    'annualized_volatility': convert_numpy_types(returns.std() * np.sqrt(252)),
                    'max_drawdown': convert_numpy_types((returns.cumsum().expanding().max() - returns.cumsum()).max())
                }
        
        if volume_cols:
            volume_col = volume_cols[0]
            volumes = pd.to_numeric(df[volume_col], errors='coerce')
            
            analysis['volume_analysis'] = {
                'avg_volume': convert_numpy_types(volumes.mean()),
                'max_volume': convert_numpy_types(volumes.max()),
                'min_volume': convert_numpy_types(volumes.min()),
                'volume_trend': 'increasing' if volumes.iloc[-5:].mean() > volumes.iloc[:5].mean() else 'decreasing'
            }
        
        return analysis
        
    except Exception as e:
        logger.error(f"Error in financial analysis: {str(e)}")
        return {'error': str(e)}

def perform_statistical_analysis(df):
    """Perform statistical analysis - simplified version matching Tkinter GUI."""
    try:
        # Exclude date/datetime columns from statistical analysis
        date_keywords = ['date', 'time', 'timestamp', 'year', 'month', 'day']
        date_cols = [col for col in df.columns 
                    if any(keyword in str(col).lower() for keyword in date_keywords) 
                    or pd.api.types.is_datetime64_any_dtype(df[col])]
        
        # Also exclude columns that are likely date components (year, month, day as integers)
        # Check if column values look like dates (years 1900-2100, months 1-12, days 1-31)
        for col in df.columns:
            if col not in date_cols and pd.api.types.is_integer_dtype(df[col]):
                unique_vals = df[col].dropna().unique()
                if len(unique_vals) > 0:
                    min_val, max_val = unique_vals.min(), unique_vals.max()
                    # Check if values look like years, months, or days
                    if (col.lower() == 'year' or (min_val >= 1900 and max_val <= 2100 and len(unique_vals) <= 100)):
                        date_cols.append(col)
                    elif (col.lower() == 'month' or (min_val >= 1 and max_val <= 12 and len(unique_vals) <= 12)):
                        date_cols.append(col)
                    elif (col.lower() == 'day' or (min_val >= 1 and max_val <= 31 and len(unique_vals) <= 31)):
                        date_cols.append(col)
        
        # Also exclude columns with very large numbers that could be timestamps
        # Unix timestamps (seconds): 1000000000-2000000000 (2001-2033)
        # Unix timestamps (milliseconds): 1000000000000-2000000000000
        # Excel serial dates: 40000-50000 (around 2009-2037)
        for col in df.columns:
            if col not in date_cols and pd.api.types.is_numeric_dtype(df[col]):
                numeric_vals = df[col].dropna()
                if len(numeric_vals) > 0:
                    min_val, max_val = numeric_vals.min(), numeric_vals.max()
                    # Check for Unix timestamps (seconds) - 1000000000 to 2000000000
                    if min_val >= 1000000000 and max_val <= 2000000000:
                        date_cols.append(col)
                        logger.debug(f"Excluding column '{col}' as Unix timestamp (seconds): range {min_val}-{max_val}")
                    # Check for Unix timestamps (milliseconds) - 1000000000000 to 2000000000000
                    elif min_val >= 1000000000000 and max_val <= 2000000000000:
                        date_cols.append(col)
                        logger.debug(f"Excluding column '{col}' as Unix timestamp (milliseconds): range {min_val}-{max_val}")
                    # Check for Excel serial dates - 40000 to 50000
                    elif min_val >= 40000 and max_val <= 50000:
                        date_cols.append(col)
                        logger.debug(f"Excluding column '{col}' as Excel serial date: range {min_val}-{max_val}")
                    # Check for very large numbers that are suspiciously date-like
                    # If values are in millions and look like they could be timestamps
                    # Be more aggressive: if all values are in this range and there are many unique values,
                    # it's likely a timestamp column (even without date-like name)
                    elif min_val > 10000000 and max_val < 1000000000:
                        unique_count = len(numeric_vals.unique())
                        total_count = len(numeric_vals)
                        # If most values are unique and in suspicious range, likely timestamps
                        if unique_count > 50 and (unique_count / total_count) > 0.8:
                            date_cols.append(col)
                            logger.debug(f"Excluding column '{col}' as potential timestamp: range {min_val:.2f}-{max_val:.2f}, {unique_count} unique values")
                        # Also exclude if column name suggests it's a date
                        elif any(keyword in str(col).lower() for keyword in ['time', 'date', 'stamp', 'epoch']):
                            date_cols.append(col)
                            logger.debug(f"Excluding column '{col}' as potential timestamp based on name and value range")
        
        # Get numeric columns excluding date columns
        numeric_cols = [col for col in df.select_dtypes(include=[np.number]).columns 
                       if col not in date_cols]
        
        # Only describe numeric columns (excluding dates)
        if len(numeric_cols) > 0:
            stats = df[numeric_cols].describe()
        else:
            stats = pd.DataFrame()
        
        # Convert to simple dictionary format
        analysis = {
            'descriptive_stats': stats.to_dict() if not stats.empty else {},
            'summary': {
                'total_rows': len(df),
                'total_columns': len(df.columns),
                'numeric_columns': len(numeric_cols),
                'excluded_date_columns': date_cols
            }
        }
        
        # Additional analysis - check for close price column (like Tkinter)
        close_col = None
        if 'close' in df.columns:
            close_col = 'close'
        elif '<CLOSE>' in df.columns:
            close_col = '<CLOSE>'
        
        if close_col:
            close_stats = {
                'Mean': float(df[close_col].mean()),
                'Median': float(df[close_col].median()),
                'Std Dev': float(df[close_col].std()),
                'Min': float(df[close_col].min()),
                'Max': float(df[close_col].max())
            }
            analysis['close_price_stats'] = close_stats
        
        return analysis
        
    except Exception as e:
        logger.error(f"Error in statistical analysis: {str(e)}")
        return {'error': str(e)}

def perform_correlation_analysis(df):
    """Perform correlation analysis - simplified version matching Tkinter GUI."""
    try:
        # Exclude date/datetime columns from correlation analysis
        date_keywords = ['date', 'time', 'timestamp', 'year', 'month', 'day']
        date_cols = [col for col in df.columns 
                    if any(keyword in str(col).lower() for keyword in date_keywords) 
                    or pd.api.types.is_datetime64_any_dtype(df[col])]
        
        # Also exclude columns that are likely date components (year, month, day as integers)
        # Check if column values look like dates (years 1900-2100, months 1-12, days 1-31)
        for col in df.columns:
            if col not in date_cols and pd.api.types.is_integer_dtype(df[col]):
                unique_vals = df[col].dropna().unique()
                if len(unique_vals) > 0:
                    min_val, max_val = unique_vals.min(), unique_vals.max()
                    # Check if values look like years, months, or days
                    if (col.lower() == 'year' or (min_val >= 1900 and max_val <= 2100 and len(unique_vals) <= 100)):
                        date_cols.append(col)
                    elif (col.lower() == 'month' or (min_val >= 1 and max_val <= 12 and len(unique_vals) <= 12)):
                        date_cols.append(col)
                    elif (col.lower() == 'day' or (min_val >= 1 and max_val <= 31 and len(unique_vals) <= 31)):
                        date_cols.append(col)
        
        # Also exclude columns with very large numbers that could be timestamps
        # Unix timestamps (seconds): 1000000000-2000000000 (2001-2033)
        # Unix timestamps (milliseconds): 1000000000000-2000000000000
        # Excel serial dates: 40000-50000 (around 2009-2037)
        for col in df.columns:
            if col not in date_cols and pd.api.types.is_numeric_dtype(df[col]):
                numeric_vals = df[col].dropna()
                if len(numeric_vals) > 0:
                    min_val, max_val = numeric_vals.min(), numeric_vals.max()
                    # Check for Unix timestamps (seconds) - 1000000000 to 2000000000
                    if min_val >= 1000000000 and max_val <= 2000000000:
                        date_cols.append(col)
                        logger.debug(f"Excluding column '{col}' as Unix timestamp (seconds): range {min_val}-{max_val}")
                    # Check for Unix timestamps (milliseconds) - 1000000000000 to 2000000000000
                    elif min_val >= 1000000000000 and max_val <= 2000000000000:
                        date_cols.append(col)
                        logger.debug(f"Excluding column '{col}' as Unix timestamp (milliseconds): range {min_val}-{max_val}")
                    # Check for Excel serial dates - 40000 to 50000
                    elif min_val >= 40000 and max_val <= 50000:
                        date_cols.append(col)
                        logger.debug(f"Excluding column '{col}' as Excel serial date: range {min_val}-{max_val}")
                    # Check for very large numbers that are suspiciously date-like
                    # If values are in millions and look like they could be timestamps
                    # Be more aggressive: if all values are in this range and there are many unique values,
                    # it's likely a timestamp column (even without date-like name)
                    elif min_val > 10000000 and max_val < 1000000000:
                        unique_count = len(numeric_vals.unique())
                        total_count = len(numeric_vals)
                        # If most values are unique and in suspicious range, likely timestamps
                        if unique_count > 50 and (unique_count / total_count) > 0.8:
                            date_cols.append(col)
                            logger.debug(f"Excluding column '{col}' as potential timestamp: range {min_val:.2f}-{max_val:.2f}, {unique_count} unique values")
                        # Also exclude if column name suggests it's a date
                        elif any(keyword in str(col).lower() for keyword in ['time', 'date', 'stamp', 'epoch']):
                            date_cols.append(col)
                            logger.debug(f"Excluding column '{col}' as potential timestamp based on name and value range")
        
        # Select numeric columns for correlation (excluding date columns)
        numeric_cols = [col for col in df.select_dtypes(include=['number']).columns 
                       if col not in date_cols]
        
        if len(numeric_cols) < 2:
            return {'error': 'Not enough numeric columns for correlation analysis'}
        
        # Calculate correlation matrix (simple approach like Tkinter)
        correlation_matrix = df[numeric_cols].corr()
        
        # Simple analysis like Tkinter GUI
        analysis = {
            'correlation_matrix': correlation_matrix.to_dict(),
            'summary': {
                'total_numeric_columns': len(numeric_cols),
                'columns_analyzed': list(numeric_cols)
            }
        }
        
        return analysis
        
    except Exception as e:
        logger.error(f"Error in correlation analysis: {str(e)}")
        return {'error': str(e)}

@analysis_bp.route('/export-analysis', methods=['POST'])
def export_analysis():
    """Export analysis results to file."""
    try:
        data = request.get_json()
        analysis_result = data.get('analysis_result')
        filename = data.get('filename', 'analysis_results')
        export_format = data.get('format', 'json')
        
        if not analysis_result:
            return jsonify({'error': 'No analysis result provided'}), 400
        
        import os
        import json
        from datetime import datetime
        
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        
        if export_format == 'json':
            export_filename = f"{filename}_{timestamp}.json"
            export_path = os.path.join(os.getcwd(), 'data', 'analysis', export_filename)
            os.makedirs(os.path.dirname(export_path), exist_ok=True)
            
            with open(export_path, 'w') as f:
                json.dump(analysis_result, f, indent=2, default=str)
        
        elif export_format == 'csv':
            # Try to convert analysis to DataFrame if possible
            export_filename = f"{filename}_{timestamp}.csv"
            export_path = os.path.join(os.getcwd(), 'data', 'analysis', export_filename)
            os.makedirs(os.path.dirname(export_path), exist_ok=True)
            
            # Create a flattened version of the analysis for CSV export
            flattened_data = []
            flatten_dict(analysis_result, flattened_data)
            
            if flattened_data:
                df = pd.DataFrame(flattened_data)
                df.to_csv(export_path, index=False)
            else:
                return jsonify({'error': 'Cannot export analysis to CSV format'}), 400
        
        else:
            return jsonify({'error': f'Unsupported export format: {export_format}'}), 400
        
        file_stat = os.stat(export_path)
        
        return jsonify({
            'message': 'Analysis exported successfully',
            'export_filename': export_filename,
            'export_path': export_path,
            'file_size': file_stat.st_size,
            'format': export_format
        })
        
    except Exception as e:
        logger.error(f"Error exporting analysis: {str(e)}")
        return jsonify({'error': str(e)}), 500

def flatten_dict(d, parent_key='', sep='_'):
    """Flatten nested dictionary for CSV export."""
    items = []
    for k, v in d.items():
        new_key = f"{parent_key}{sep}{k}" if parent_key else k
        if isinstance(v, dict):
            items.extend(flatten_dict(v, new_key, sep=sep).items())
        else:
            items.append((new_key, v))
    return dict(items)
